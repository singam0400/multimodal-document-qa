# -*- coding: utf-8 -*-
"""Multimodal QA.ipynb

Automatically generated by Colab.

"""

# @title 2. Import Libraries and Configure API Key
import google.generativeai as genai
from PIL import Image
import io
import os
import fitz # PyMuPDF for PDF handling
from google.colab import userdata # For secure API key storage
from google.colab import files # For file upload in Colab

print("Libraries imported.")

import os

os.environ["GOOGLE_API_KEY"] = ""

try:
    API_KEY = userdata.get('GOOGLE_API_KEY')
    genai.configure(api_key=API_KEY)
    print("Gemini API key configured from Colab Secrets.")
except userdata.SecretNotFoundError:
    print("WARNING: GOOGLE_API_KEY not found in Colab Secrets.")
    print("Please add your API key to Colab Secrets for secure usage.")
    print("If you want to proceed for quick testing, you can uncomment and hardcode below (NOT recommended for production):")
    # genai.configure(api_key="YOUR_YOUR_GOOGLE_API_KEY_HERE")
    # print("API key hardcoded (for testing only).")
except Exception as e:
    print(f"An error occurred while configuring API key: {e}")

# Initialize the Gemini 1.5 Flash model
try:
    model = genai.GenerativeModel('gemini-1.5-flash-latest')
    print("Gemini 1.5 Flash model initialized.")
except Exception as e:
    print(f"Error initializing Gemini model: {e}. Please check your API key and network connection.")

# @title 3. Document Loading and Preprocessing Functions
def load_document_images(file_path):
    """
    Loads a document (image or PDF) and returns a list of PIL Image objects.
    Each page of a PDF will be converted into a separate image.
    """
    images = []
    file_path = str(file_path) # Ensure file_path is a string

    if file_path.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff', '.bmp', '.gif')):
        try:
            # Ensure image is RGB for consistency with various model inputs
            images.append(Image.open(file_path).convert("RGB"))
            print(f"Loaded image: {file_path}")
        except FileNotFoundError:
            print(f"Error: Image file not found at {file_path}")
        except Exception as e:
            print(f"Error loading image {file_path}: {e}")
    elif file_path.lower().endswith('.pdf'):
        try:
            doc = fitz.open(file_path)
            for page_num in range(doc.page_count):
                page = doc.load_page(page_num)
                # Render at a higher resolution (e.g., 2x) for better OCR performance
                # Adjust matrix values if images are too large/small for your needs
                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))
                img_data = pix.tobytes("png")
                img = Image.open(io.BytesIO(img_data)).convert("RGB")
                images.append(img)
                print(f"Converted page {page_num + 1}/{doc.page_count} of PDF to image.")
            doc.close()
        except FileNotFoundError:
            print(f"Error: PDF file not found at {file_path}")
        except Exception as e:
            print(f"Error loading PDF {file_path}: {e}")
    else:
        print(f"Error: Unsupported file format for {file_path}. Please provide an image or PDF (PNG, JPG, JPEG, TIFF, BMP, GIF, PDF).")
    return images

def save_uploaded_file(filename, file_bytes):
    temp_dir = "/content/uploaded_files"
    os.makedirs(temp_dir, exist_ok=True)
    file_path = os.path.join(temp_dir, filename)
    with open(file_path, 'wb') as f:
        f.write(file_bytes)
    print(f"File saved to: {file_path}")
    return file_path

# @title 4. Multimodal QA Function with Prompt Engineering
def ask_gemini_about_document(document_images, question):

    if not document_images:
        return "Error: No document images provided for the query."

    # --- Prompt Engineering Component ---
    # This prompt is designed to:
    # 1. Clearly instruct the model to use ONLY the provided document.
    # 2. Guide it to state when information is NOT present.
    # 3. Encourage concise, direct answers.
    # 4. Potentially encourage a "Chain-of-Thought" if the question implies it,
    #    though for simple QA, a direct answer is often preferred.
    #    For complex scientific papers, you'd add more explicit CoT steps here.

    initial_instruction = (
        "You are an expert document analysis assistant. "
        "Your task is to answer the user's question *strictly based* on the content "
        "of the provided document images. Do NOT use any external knowledge. "
        "If the answer is not explicitly present in the document, state 'Information not found in the document.'. "
        "Be concise and direct in your answer.\n\n"
    )

    # Combine instruction, question, and an explicit marker for the answer
    # This structure helps the model understand where the answer should begin
    user_query_prompt = f"Question: {question}\n\nAnswer:"

    # Prepare the content for the model.
    # The initial text prompt sets the stage, followed by all document images.
    parts = [initial_instruction, user_query_prompt]

    for i, img in enumerate(document_images):
        # Adding a label for each image might help in very complex multi-page scenarios,
        # but for typical QA, simply providing the images is often sufficient.
        # You could add: parts.append(f"Document Page {i+1}:") before the image.
        parts.append(img)

    try:
        print(f"\nSending {len(document_images)} image(s) and engineered prompt to Gemini 1.5 Flash...")
        # Use a low temperature for more deterministic and factual answers
        response = model.generate_content(parts, generation_config={"temperature": 0.1})
        return response.text
    except genai.types.BlockedPromptException as e:
        print(f"Prompt was blocked due to safety concerns: {e}")
        return "The prompt was blocked due to safety concerns. Please try rephrasing your question or check the document content."
    except Exception as e:
        print(f"An error occurred during Gemini API call: {e}")
        return f"An error occurred during generation: {e}"

# @title 5. Main Execution Block (User Interaction)
print("--- Multimodal Document QA with Prompt Engineering ---")
print("Please upload your scanned document (image: .png, .jpg, .jpeg, .tiff, .bmp, .gif; or PDF) when prompted.")

uploaded_files = files.upload()  # Colab's file upload widget

# Initialize file_path to None
uploaded_file_path = None

if uploaded_files:
    # Assuming only one file is uploaded
    uploaded_file_name = list(uploaded_files.keys())[0]
    uploaded_file_bytes = uploaded_files[uploaded_file_name]

    # Fix: Pass both filename and file bytes
    uploaded_file_path = save_uploaded_file(uploaded_file_name, uploaded_file_bytes)

    if uploaded_file_path:
        document_images = load_document_images(uploaded_file_path)

        if document_images:
            user_question = input("\nEnter your question about the document: ")

            if user_question:
                print("\nProcessing your request with Gemini 1.5 Flash. Please wait...")
                answer = ask_gemini_about_document(document_images, user_question)
                print("\n--- AI Answer ---")
                print(answer)
                print("-----------------\n")
            else:
                print("No question entered. Exiting.")
        else:
            print("Failed to load document images. Cannot proceed with QA.")
    else:
        print("Failed to save uploaded file. Cannot proceed with QA.")
else:
    print("No file uploaded. Exiting.")

# Optional: Clean up uploaded files and directory
try:
    if uploaded_file_path and os.path.exists(uploaded_file_path):
        os.remove(uploaded_file_path)
        print(f"Cleaned up uploaded file: {uploaded_file_path}")
    temp_dir = "/content/uploaded_files"
    if os.path.exists(temp_dir) and not os.listdir(temp_dir):
        os.rmdir(temp_dir)
        print(f"Cleaned up temporary directory: {temp_dir}")
except Exception as e:
    print(f"Error during cleanup: {e}")

# @title 6. Evaluation Metrics
# This cell is dedicated to discussing and demonstrating evaluation.

print("--- Evaluation Metrics for Document QA System ---")
print("To truly evaluate a Document QA system, you need a robust framework.")

print("\n**--- Manual Spot Check (Limited Demonstration) ---**")
print("This is a simple interactive check for the *last question asked* in the previous cell.")

if 'answer' in locals() and answer is not None:
    run_manual_check = input("Do you want to run a manual spot check for the last question? (yes/no): ").lower()
    if run_manual_check == 'yes':
        ground_truth_answer = input("Enter the EXACT correct answer for the last question (from the document): ")
        if ground_truth_answer:
            # Basic exact match for demonstration
            # You'd use more sophisticated NLP metrics in a real project
            llm_cleaned = answer.strip().lower()
            gt_cleaned = ground_truth_answer.strip().lower()

            print(f"\nLast Question: '{user_question}'")
            print(f"LLM Answer:   '{answer}'")
            print(f"Ground Truth: '{ground_truth_answer}'")

            if llm_cleaned == gt_cleaned:
                print("Manual Check Result: EXACT MATCH! (This is a very strict metric).")
            else:
                print("Manual Check Result: NO EXACT MATCH.")
            print("Note: Exact match is often too strict for LLM outputs due to phrasing variations. F1-score is typically preferred.")
        else:
            print("No ground truth answer provided for manual check.")
    else:
        print("Manual spot check skipped.")
else:
    print("No previous LLM answer found to perform a manual spot check. Please run the QA cell first.")

# @title 6. Evaluation Metrics (Quantitative + Manual)
# Evaluate the LLM-generated answer against a reference answer using standard NLP metrics

import numpy as np
from sklearn.metrics import f1_score
from rouge_score import rouge_scorer

print("--- Evaluation Metrics for Document QA System ---\n")

# Check for last generated answer
if 'answer' not in locals() or answer is None:
    print("No previous answer found. Please run the QA cell first.")
else:
    print(f"LLM Answer: {answer}")
    ref = input("\nEnter the ground truth/reference answer: ").strip()

    if not ref:
        print("No ground truth provided. Skipping evaluation.")
    else:
        # Tokenize using simple whitespace split
        pred_tokens = answer.lower().split()
        ref_tokens = ref.lower().split()

        # ---- Exact Match (EM)
        exact_match = int(answer.strip().lower() == ref.strip().lower())
        print(f"\nExact Match (EM): {exact_match}")

        # ---- F1 Score (Token Overlap)
        all_tokens = list(set(pred_tokens + ref_tokens))
        pred_binary = [1 if token in pred_tokens else 0 for token in all_tokens]
        ref_binary  = [1 if token in ref_tokens else 0 for token in all_tokens]
        f1 = f1_score(ref_binary, pred_binary, average='binary')
        print(f"F1 Score (token overlap): {f1:.4f}")

        # ---- ROUGE-L (Longest Common Subsequence)
        scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
        scores = scorer.score(ref, answer)
        rouge_l = scores['rougeL'].fmeasure
        print(f"ROUGE-L Score: {rouge_l:.4f}")

        # ---- Summary
        print("\nSummary:")
        print(f" - EM:       {exact_match}")
        print(f" - F1 Score: {f1:.4f}")
        print(f" - ROUGE-L:  {rouge_l:.4f}")

        # Optional: Manual comment
        comment = input("\nAny human judgment or notes (optional): ")
        if comment:
            print(f"Note: {comment}")

# @title 7. Cleanup Temporary Files
# This cell is for cleaning up files after the entire notebook has run.
last_document_path = uploaded_file_path
try:
    if 'last_document_path' in locals() and last_document_path and os.path.exists(last_document_path):
        os.remove(last_document_path)
        print(f"Cleaned up uploaded file: {last_document_path}")
    temp_dir = "/content/uploaded_files"
    if os.path.exists(temp_dir):
        # rmdir only works if the directory is empty. For robust cleanup, use shutil.rmtree.
        if not os.listdir(temp_dir): # Check if directory is empty
            os.rmdir(temp_dir)
            print(f"Cleaned up temporary directory: {temp_dir}")
        else:
            print(f"Temporary directory {temp_dir} not empty, skipping rmdir.")
except Exception as e:
    print(f"Error during cleanup: {e}")

